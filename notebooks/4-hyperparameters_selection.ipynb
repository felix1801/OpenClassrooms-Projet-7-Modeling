{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\04_feature\\\\second_features_selection.csv\"\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 18\n",
    "cost_fn = 10\n",
    "cost_fp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment and tracking URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns/534552444611581124', creation_time=1725124865324, experiment_id='534552444611581124', last_update_time=1725124865324, lifecycle_stage='active', name='Credit_Scoring_Model', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(uri=\"file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns\")\n",
    "mlflow.set_experiment(\"Credit_Scoring_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 12)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(data_path)\n",
    "data = raw_data.astype('float16')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the 10 000 first rows for testing\n",
    "# data = data[:10000]\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"TARGET\", axis=1)\n",
    "y = data[\"TARGET\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and search best parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, param_grid):\n",
    "    # Initialisation du modèle\n",
    "    model = lgb.LGBMClassifier()\n",
    "\n",
    "    # Initialisation de GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a work cost function\n",
    "def cost_metric(y_true, y_pred, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    _, fp, fn, _ = confusion_matrix(y_true, y_pred).ravel() # c'est bien arg 2 et 3 pour fp et fn ? -> oui\n",
    "    cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    _, _, thresholds = roc_curve(y_true, y_proba)\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba <= threshold).astype(int)\n",
    "        cost = cost_metric(y_true, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold2(y_true, y_proba, cost_fn=cost_fn):\n",
    "    y_true = np.array(y_true)\n",
    "    _, _, thresholds = roc_curve(y_true, y_proba)\n",
    "\n",
    "    scores = []\n",
    "    for threshold in thresholds:        \n",
    "        y_pred = (y_proba <= threshold).astype(int) \n",
    "        \n",
    "        false_positives = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        score = -(false_positives*cost_fp + cost_fn*false_negatives)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta_score(y_true, y_pred, beta=1):\n",
    "    tp, fp, fn, _ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return f_beta\n",
    "\n",
    "# bonne métrique pour les déséquilibres de coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_scoring(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    thresholds = np.linspace(0.02, 0.5, 50)\n",
    "    costs = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba <= threshold).astype(int)\n",
    "        cost = 0 if (y_true==y_pred).all() else (cost_fp if (y_true==0).all() and (y_pred==1).all() else cost_fn)\n",
    "        costs.append(cost)\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(best_model, X_test, y_test):\n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    time1 = time.time()\n",
    "    # optimal_threshold = optimize_threshold(y_test, y_proba) # Production\n",
    "    optimal_threshold = 0.2 # Test\n",
    "    print(f\"Time for optimal threshold: {time.time()-time1}\")\n",
    "    print(f\"optimal_threshold1: {optimal_threshold}\")\n",
    "\n",
    "    time8 = time.time()\n",
    "    optimal_threshold2 = optimize_threshold2(y_test, y_proba, cost_fn)\n",
    "    print(f\"\\nTime for optimal threshold2: {time.time()-time8}\")\n",
    "    print(f\"optimal_threshold2: {optimal_threshold2}\")\n",
    "\n",
    "    time9 = time.time()\n",
    "    # manual_threshold = manual_scoring(y_test, y_proba, cost_fn, cost_fp) # Production\n",
    "    manual_threshold = 0.2 # Test\n",
    "    print(f\"Time for manual threshold: {time.time()-time9}\")\n",
    "    print(f\"manual_threshold: {manual_threshold}\")\n",
    "\n",
    "    y_pred = (y_proba <= manual_threshold).astype(int)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    time2 = time.time()\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\nTime for auc roc: {time.time()-time2}\")\n",
    "\n",
    "    time3 = time.time()\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    print(f\"Time for precision recall: {time.time()-time3}\")\n",
    "    \n",
    "    time4 = time.time()\n",
    "    auc_pr = auc(recall, precision) # Area Under the Precision-Recall Curve\n",
    "    print(f\"Time for auc pr: {time.time()-time4}\")\n",
    "\n",
    "    time5 = time.time()\n",
    "    cost = cost_metric(y_test, y_pred)\n",
    "    print(f\"Time for cost: {time.time()-time5}\")\n",
    "\n",
    "    time6 = time.time()\n",
    "    f1 = f1_score(y_test, y_pred) # F1 score\n",
    "    print(f\"Time for f1: {time.time()-time6}\")\n",
    "\n",
    "    time7 = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Time for accuracy: {time.time()-time7}\")\n",
    "\n",
    "    time10 = time.time()\n",
    "    f_beta = f_beta_score(y_test, y_pred)\n",
    "    print(f\"Time for f beta: {time.time()-time10}\")\n",
    "\n",
    "    return accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log experiment data into MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.334168964652795\n"
     ]
    }
   ],
   "source": [
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "base_scale_pos_weight = n_negative / n_positive\n",
    "print(base_scale_pos_weight)\n",
    "# base_scale_pos_weight = 11.334168964652795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General advice for efficient testing:\n",
    "\n",
    "Use a smaller subset of your data for initial tests to speed up the process.  \n",
    "Focus on parameters that typically have the most impact first: learning_rate, num_leaves, and n_estimators.\n",
    "Keep other parameters at their default values while testing these key parameters.  \n",
    "Once you've found good values for the key parameters, move on to fine-tuning the others.  \n",
    "Remember that parameters can interact with each other, so you may need to revisit some parameters after changing others.  \n",
    "\n",
    "By following this strategy, you can quickly narrow down the most promising parameter values with just two initial tests for each parameter. This focused approach will help you efficiently find a good set of parameters for your LightGBM model, considering your specific requirements for handling imbalanced data and asymmetric misclassification costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grid with 2 or 3 options on each for LGBMClassifier\n",
    "param_grid = {\n",
    "    'num_leaves': [60, 44], # 255, 127, 31\n",
    "    'learning_rate': [0.05, 0.1, ], # 0.01, 0.3\n",
    "    'n_estimators': [150, 200 ], # 1000, 500, 100\n",
    "    # 'max_depth': [6, 12, ], # 20\n",
    "    # 'min_child_samples': [20, 50, ], # 100\n",
    "    # 'subsample': [0.6, 0.8, ], # 1.0\n",
    "    # 'colsample_bytree': [0.6, 0.8, ], # 1.0\n",
    "    # 'reg_alpha': [0, 1, ], # 10\n",
    "    # 'reg_lambda': [0, 1, ], # 10\n",
    "    'scale_pos_weight': [\n",
    "        base_scale_pos_weight, \n",
    "        # base_scale_pos_weight * 5, \n",
    "        # base_scale_pos_weight * 10,\n",
    "        ]\n",
    "}\n",
    "\n",
    "# scale_pos_weight y a t il auto comme param?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de référence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (non) pour tout le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 0.031264305114746094\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.07474446296691895\n",
      "manual_threshold: 0.02\n",
      "\n",
      "Time for auc roc: 0.03842282295227051\n",
      "Time for precision recall: 0.015636682510375977\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.5618212223052979\n",
      "Time for f1: 0.08158016204833984\n",
      "Time for accuracy: 0.016668319702148438\n",
      "Time for f beta: 0.4590303897857666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/19 19:20:43 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from modeling.pipelines.utils.always_one_classifier import AlwaysOneClassifier\n",
    "\n",
    "with mlflow.start_run():  \n",
    "    always_one_model = AlwaysOneClassifier()\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(always_one_model, X_test, y_test)\n",
    "    \n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"Baseline model\")\n",
    "    mlflow.set_tag(\"Model\", \"AlwaysOneClassifier\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(always_one_model, \"AlwaysOne Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 (oui) pour tout le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 0.03454089164733887\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.0914607048034668\n",
      "manual_threshold: 0.02\n",
      "\n",
      "Time for auc roc: 0.03906059265136719\n",
      "Time for precision recall: 0.018339872360229492\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.45151257514953613\n",
      "Time for f1: 0.058870553970336914\n",
      "Time for accuracy: 0.015625953674316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Local\\Temp\\ipykernel_16224\\3245958202.py:4: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall = tp / (tp + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for f beta: 0.5461218357086182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/19 19:21:06 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from modeling.pipelines.utils.always_zero_classifier import AlwaysZeroClassifier\n",
    "\n",
    "with mlflow.start_run():  \n",
    "    always_zero_model = AlwaysZeroClassifier()\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(always_zero_model, X_test, y_test)\n",
    "    \n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"Baseline model\")\n",
    "    mlflow.set_tag(\"Model\", \"AlwaysOneClassifier\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(always_zero_model, \"AlwaysZero Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Start an MLflow run\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m## Run experiment\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     grid_search \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     best_model \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Assess model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X_train, y_train, param_grid)\u001b[0m\n\u001b[0;32m      6\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:976\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[1;32m--> 976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:416\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         (\n\u001b[0;32m    411\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    414\u001b[0m     )\n\u001b[1;32m--> 416\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:147\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    145\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    146\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m--> 147\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_test_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_not\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:809\u001b[0m, in \u001b[0;36mStratifiedKFold._iter_test_masks\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_iter_test_masks\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 809\u001b[0m     test_folds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_test_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits):\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m test_folds \u001b[38;5;241m==\u001b[39m i\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:760\u001b[0m, in \u001b[0;36mStratifiedKFold._make_test_folds\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSupported target types are: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    754\u001b[0m             allowed_target_types, type_of_target_y\n\u001b[0;32m    755\u001b[0m         )\n\u001b[0;32m    756\u001b[0m     )\n\u001b[0;32m    758\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y)\n\u001b[1;32m--> 760\u001b[0m _, y_idx, y_inv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;66;03m# y_inv encodes y according to lexicographic order. We invert y_idx to\u001b[39;00m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;66;03m# map the classes so that they are encoded by order of appearance:\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;66;03m# 0 represents the first label appearing in y, 1 the second, etc.\u001b[39;00m\n\u001b[0;32m    764\u001b[0m _, class_perm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_idx, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\arraysetops.py:333\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    330\u001b[0m optional_indices \u001b[38;5;241m=\u001b[39m return_index \u001b[38;5;129;01mor\u001b[39;00m return_inverse\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_indices:\n\u001b[1;32m--> 333\u001b[0m     perm \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmergesort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquicksort\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    ## Run experiment\n",
    "    # Train model\n",
    "    grid_search = train_model(X_train, y_train, param_grid)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Assess model\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold = assess_model(best_model, X_test, y_test)\n",
    "\n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"LGBM hyperparameters tuning\", \"second try\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics (accuracy and AUC-ROC)\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinage des hyperparamètres en fonction des seuils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19945, number of negative: 226060\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081076 -> initscore=-2.427822\n",
      "[LightGBM] [Info] Start training from score -2.427822\n",
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 10.61282992362976\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.0\n",
      "manual_threshold: 0.2\n",
      "\n",
      "Time for auc roc: 0.12268614768981934\n",
      "Time for precision recall: 0.03197193145751953\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.5494816303253174\n",
      "Time for f1: 0.07756805419921875\n",
      "Time for accuracy: 0.01603531837463379\n",
      "Time for f beta: 0.4054219722747803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 21:47:57 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def custom_scorer(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    optimal_threshold = optimize_threshold2(y_true, y_proba, cost_fn, cost_fp)\n",
    "    y_pred = (y_proba <= optimal_threshold).astype(int)\n",
    "    return -cost_metric(y_true, y_pred, cost_fn, cost_fp)  # Négatif car GridSearchCV maximise le score\n",
    "\n",
    "def evaluate_model(model, X, y, param_grid, cv=5):\n",
    "    # Définir le scorer personnalisé\n",
    "    scorer = make_scorer(custom_scorer, needs_proba=True, greater_is_better=False)\n",
    "    \n",
    "    # Configurer la recherche sur grille avec validation croisée\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=KFold(n_splits=cv, shuffle=True, random_state=random_state),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Effectuer la recherche sur grille\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = lgb.LGBMClassifier()\n",
    "    \n",
    "    # Utilisation\n",
    "    best_model, best_params, best_score = evaluate_model(model, X_train, y_train, param_grid)\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(best_model, X_test, y_test)\n",
    "    \n",
    "        ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"hyper params and score finetuning\")\n",
    "    mlflow.set_tag(\"Model\", \"LGBM\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "    mlflow.set_tag(\"y pred __ y proba X threshold\", \"<=\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"best_params\", best_params)\n",
    "    mlflow.log_param(\"best_score\", best_score)\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur modèle sauvegardé dans: C:\\Users\\Z478SG\\Desktop\\Ecole\\OpenClassrooms-Projet-7\\modeling\\data\\06_models\\latest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "from modeling.pipelines.utils.custom_threshold_model import CustomThresholdModel\n",
    "\n",
    "# Créer un client MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Récupérer tous les runs de l'expériment\n",
    "runs = client.search_runs(experiment_ids=['534552444611581124'])\n",
    "\n",
    "best_run = runs[0]\n",
    "\n",
    "model_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Ecole\", \"OpenClassrooms-Projet-7\", \"modeling\", \"data\", \"06_models\", \"latest\")\n",
    "\n",
    "best_model = mlflow.sklearn.load_model(f\"runs:/{best_run.info.run_id}/model\")\n",
    "\n",
    "# créer un model custom qui a une fonction .predict() qui contient la valeur de la métrique \"Manual Threshold\". Cette fonction utilise .predict_proba(), comparer le résultat au seuil et retourne 1 ou 0 en fonction de si la probabilité est supérieure ou inférieure à la valeur de la métrique \"Manual Threshold\" \n",
    "manual_threshold = client.get_metric_history(best_run.info.run_id, \"Manual Threshold\")[0].value\n",
    "\n",
    "custom_model = CustomThresholdModel(best_model, manual_threshold)\n",
    "\n",
    "mlflow.sklearn.save_model(custom_model, model_path)\n",
    "\n",
    "print(f\"Meilleur modèle sauvegardé dans: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without training\n",
    "\n",
    "import pickle\n",
    "with open('C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\06_models\\\\model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # Assess model\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(best_model, X_test, y_test)\n",
    "\n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"LGBM hyperparameters tuning\", \"second try\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    # mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics (accuracy and AUC-ROC)\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
