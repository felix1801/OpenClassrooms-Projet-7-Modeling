{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\04_feature\\\\second_features_selection.csv\"\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 18\n",
    "cost_fn = 10\n",
    "cost_fp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment and tracking URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns/534552444611581124', creation_time=1725124865324, experiment_id='534552444611581124', last_update_time=1725124865324, lifecycle_stage='active', name='Credit_Scoring_Model', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(uri=\"file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns\")\n",
    "mlflow.set_experiment(\"Credit_Scoring_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(data_path)\n",
    "data = raw_data.astype('float16')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the 10 000 first rows for testing\n",
    "# data = data[:10000]\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"TARGET\", axis=1)\n",
    "y = data[\"TARGET\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and search best parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, param_grid):\n",
    "    # Initialisation du modèle\n",
    "    model = lgb.LGBMClassifier()\n",
    "\n",
    "    # Initialisation de GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a work cost function\n",
    "def cost_metric(y_true, y_pred, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    _, fp, fn, _ = confusion_matrix(y_true, y_pred).ravel() # c'est bien arg 2 et 3 pour fp et fn ? -> oui\n",
    "    cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    _, _, thresholds = roc_curve(y_true, y_proba)\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba <= threshold).astype(int)\n",
    "        cost = cost_metric(y_true, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold2(y_true, y_proba, cost_fn=cost_fn):\n",
    "    y_true = np.array(y_true)\n",
    "    _, _, thresholds = roc_curve(y_true, y_proba)\n",
    "\n",
    "    scores = []\n",
    "    for threshold in thresholds:        \n",
    "        y_pred = (y_proba <= threshold).astype(int) \n",
    "        \n",
    "        false_positives = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        false_negatives = np.sum((y_true == 1) & (y_pred == 0))\n",
    "        \n",
    "        score = -(false_positives*cost_fp + cost_fn*false_negatives)\n",
    "        scores.append(score)\n",
    "    \n",
    "    optimal_idx = np.argmax(scores)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_beta_score(y_true, y_pred, beta=1):\n",
    "    tp, fp, fn, _ = confusion_matrix(y_true, y_pred).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_beta = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return f_beta\n",
    "\n",
    "# bonne métrique pour les déséquilibres de coût."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_scoring(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    thresholds = np.linspace(0.02, 0.5, 50)\n",
    "    costs = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba <= threshold).astype(int)\n",
    "        cost = 0 if (y_true==y_pred).all() else (cost_fp if (y_true==0).all() and (y_pred==1).all() else cost_fn)\n",
    "        costs.append(cost)\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(best_model, X_test, y_test):\n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    time1 = time.time()\n",
    "    # optimal_threshold = optimize_threshold(y_test, y_proba) # Production\n",
    "    optimal_threshold = 0.2 # Test\n",
    "    print(f\"Time for optimal threshold: {time.time()-time1}\")\n",
    "    print(f\"optimal_threshold1: {optimal_threshold}\")\n",
    "\n",
    "    time8 = time.time()\n",
    "    optimal_threshold2 = optimize_threshold2(y_test, y_proba, cost_fn)\n",
    "    print(f\"\\nTime for optimal threshold2: {time.time()-time8}\")\n",
    "    print(f\"optimal_threshold2: {optimal_threshold2}\")\n",
    "\n",
    "    time9 = time.time()\n",
    "    manual_threshold = manual_scoring(y_test, y_proba, cost_fn, cost_fp)\n",
    "    print(f\"Time for manual threshold: {time.time()-time9}\")\n",
    "    print(f\"manual_threshold: {manual_threshold}\")\n",
    "\n",
    "    y_pred = (y_proba <= manual_threshold).astype(int)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    time2 = time.time()\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\nTime for auc roc: {time.time()-time2}\")\n",
    "\n",
    "    time3 = time.time()\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    print(f\"Time for precision recall: {time.time()-time3}\")\n",
    "    \n",
    "    time4 = time.time()\n",
    "    auc_pr = auc(recall, precision) # Area Under the Precision-Recall Curve\n",
    "    print(f\"Time for auc pr: {time.time()-time4}\")\n",
    "\n",
    "    time5 = time.time()\n",
    "    cost = cost_metric(y_test, y_pred)\n",
    "    print(f\"Time for cost: {time.time()-time5}\")\n",
    "\n",
    "    time6 = time.time()\n",
    "    f1 = f1_score(y_test, y_pred) # F1 score\n",
    "    print(f\"Time for f1: {time.time()-time6}\")\n",
    "\n",
    "    time7 = time.time()\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Time for accuracy: {time.time()-time7}\")\n",
    "\n",
    "    time10 = time.time()\n",
    "    f_beta = f_beta_score(y_test, y_pred)\n",
    "    print(f\"Time for f beta: {time.time()-time10}\")\n",
    "\n",
    "    return accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log experiment data into MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.334168964652795\n"
     ]
    }
   ],
   "source": [
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "base_scale_pos_weight = n_negative / n_positive\n",
    "print(base_scale_pos_weight)\n",
    "# base_scale_pos_weight = 11.334168964652795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General advice for efficient testing:\n",
    "\n",
    "Use a smaller subset of your data for initial tests to speed up the process.  \n",
    "Focus on parameters that typically have the most impact first: learning_rate, num_leaves, and n_estimators.\n",
    "Keep other parameters at their default values while testing these key parameters.  \n",
    "Once you've found good values for the key parameters, move on to fine-tuning the others.  \n",
    "Remember that parameters can interact with each other, so you may need to revisit some parameters after changing others.  \n",
    "\n",
    "By following this strategy, you can quickly narrow down the most promising parameter values with just two initial tests for each parameter. This focused approach will help you efficiently find a good set of parameters for your LightGBM model, considering your specific requirements for handling imbalanced data and asymmetric misclassification costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grid with 2 or 3 options on each for LGBMClassifier\n",
    "param_grid = {\n",
    "    'num_leaves': [60, 44], # 255, 127, 31\n",
    "    'learning_rate': [0.05, 0.1, ], # 0.01, 0.3\n",
    "    'n_estimators': [150, 200 ], # 1000, 500, 100\n",
    "    # 'max_depth': [6, 12, ], # 20\n",
    "    # 'min_child_samples': [20, 50, ], # 100\n",
    "    # 'subsample': [0.6, 0.8, ], # 1.0\n",
    "    # 'colsample_bytree': [0.6, 0.8, ], # 1.0\n",
    "    # 'reg_alpha': [0, 1, ], # 10\n",
    "    # 'reg_lambda': [0, 1, ], # 10\n",
    "    'scale_pos_weight': [\n",
    "        base_scale_pos_weight, \n",
    "        # base_scale_pos_weight * 5, \n",
    "        # base_scale_pos_weight * 10,\n",
    "        ]\n",
    "}\n",
    "\n",
    "# scale_pos_weight y a t il auto comme param?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle de référence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (non) pour tout le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 0.031264305114746094\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.07474446296691895\n",
      "manual_threshold: 0.02\n",
      "\n",
      "Time for auc roc: 0.03842282295227051\n",
      "Time for precision recall: 0.015636682510375977\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.5618212223052979\n",
      "Time for f1: 0.08158016204833984\n",
      "Time for accuracy: 0.016668319702148438\n",
      "Time for f beta: 0.4590303897857666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/19 19:20:43 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from modeling.pipelines.utils.always_one_classifier import AlwaysOneClassifier\n",
    "\n",
    "with mlflow.start_run():  \n",
    "    always_one_model = AlwaysOneClassifier()\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(always_one_model, X_test, y_test)\n",
    "    \n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"Baseline model\")\n",
    "    mlflow.set_tag(\"Model\", \"AlwaysOneClassifier\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(always_one_model, \"AlwaysOne Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 (oui) pour tout le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 0.03454089164733887\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.0914607048034668\n",
      "manual_threshold: 0.02\n",
      "\n",
      "Time for auc roc: 0.03906059265136719\n",
      "Time for precision recall: 0.018339872360229492\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.45151257514953613\n",
      "Time for f1: 0.058870553970336914\n",
      "Time for accuracy: 0.015625953674316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Local\\Temp\\ipykernel_16224\\3245958202.py:4: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall = tp / (tp + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for f beta: 0.5461218357086182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/19 19:21:06 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from modeling.pipelines.utils.always_zero_classifier import AlwaysZeroClassifier\n",
    "\n",
    "with mlflow.start_run():  \n",
    "    always_zero_model = AlwaysZeroClassifier()\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(always_zero_model, X_test, y_test)\n",
    "    \n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"Baseline model\")\n",
    "    mlflow.set_tag(\"Model\", \"AlwaysOneClassifier\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(always_zero_model, \"AlwaysZero Baseline Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[       inf 0.92454165 0.91842248 ... 0.05712654 0.05708317 0.01899781]\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    ## Run experiment\n",
    "    # Train model\n",
    "    grid_search = train_model(X_train, y_train, param_grid)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Assess model\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold = assess_model(best_model, X_test, y_test)\n",
    "\n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"LGBM hyperparameters tuning\", \"second try\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics (accuracy and AUC-ROC)\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affinage des hyperparamètres en fonction des seuils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_scorer.py:610: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Z478SG\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 19945, number of negative: 226060\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024229 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2628\n",
      "[LightGBM] [Info] Number of data points in the train set: 246005, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.081076 -> initscore=-2.427822\n",
      "[LightGBM] [Info] Start training from score -2.427822\n",
      "Time for optimal threshold: 0.0\n",
      "optimal_threshold1: 0.2\n",
      "\n",
      "Time for optimal threshold2: 9.405752182006836\n",
      "optimal_threshold2: inf\n",
      "Time for manual threshold: 0.049324989318847656\n",
      "manual_threshold: 0.02\n",
      "\n",
      "Time for auc roc: 0.031294822692871094\n",
      "Time for precision recall: 0.015625476837158203\n",
      "Time for auc pr: 0.0\n",
      "Time for cost: 0.30652356147766113\n",
      "Time for f1: 0.04688906669616699\n",
      "Time for accuracy: 0.015634775161743164\n",
      "Time for f beta: 0.4062659740447998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 20:21:37 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def custom_scorer(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    optimal_threshold = optimize_threshold2(y_true, y_proba, cost_fn, cost_fp)\n",
    "    y_pred = (y_proba <= optimal_threshold).astype(int)\n",
    "    return -cost_metric(y_true, y_pred, cost_fn, cost_fp)  # Négatif car GridSearchCV maximise le score\n",
    "\n",
    "def evaluate_model(model, X, y, param_grid, cv=5):\n",
    "    # Définir le scorer personnalisé\n",
    "    scorer = make_scorer(custom_scorer, needs_proba=True, greater_is_better=False)\n",
    "    \n",
    "    # Configurer la recherche sur grille avec validation croisée\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scorer,\n",
    "        cv=KFold(n_splits=cv, shuffle=True, random_state=random_state),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Effectuer la recherche sur grille\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = lgb.LGBMClassifier()\n",
    "    \n",
    "    # Utilisation\n",
    "    best_model, best_params, best_score = evaluate_model(model, X_train, y_train, param_grid)\n",
    "\n",
    "    # Évaluation finale sur l'ensemble de test\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(best_model, X_test, y_test)\n",
    "    \n",
    "        ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Process\", \"hyper params and score finetuning\")\n",
    "    mlflow.set_tag(\"Model\", \"LGBM\")\n",
    "    mlflow.set_tag(\"Data\", \"full\")\n",
    "    mlflow.set_tag(\"y pred __ y proba X threshold\", \"<=\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_param(\"best_params\", best_params)\n",
    "    mlflow.log_param(\"best_score\", best_score)\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur modèle sauvegardé dans: C:\\Users\\Z478SG\\Desktop\\Ecole\\OpenClassrooms-Projet-7\\modeling\\data\\06_models\\latest\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "from modeling.pipelines.utils.custom_threshold_model import CustomThresholdModel\n",
    "\n",
    "# Créer un client MLflow\n",
    "client = MlflowClient()\n",
    "\n",
    "# Récupérer tous les runs de l'expériment\n",
    "runs = client.search_runs(experiment_ids=['534552444611581124'])\n",
    "\n",
    "best_run = runs[0]\n",
    "\n",
    "model_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Ecole\", \"OpenClassrooms-Projet-7\", \"modeling\", \"data\", \"06_models\", \"latest\")\n",
    "\n",
    "best_model = mlflow.sklearn.load_model(f\"runs:/{best_run.info.run_id}/model\")\n",
    "\n",
    "# créer un model custom qui a une fonction .predict() qui contient la valeur de la métrique \"Manual Threshold\". Cette fonction utilise .predict_proba(), comparer le résultat au seuil et retourne 1 ou 0 en fonction de si la probabilité est supérieure ou inférieure à la valeur de la métrique \"Manual Threshold\" \n",
    "manual_threshold = client.get_metric_history(best_run.info.run_id, \"Manual Threshold\")[0].value\n",
    "\n",
    "custom_model = CustomThresholdModel(best_model, manual_threshold)\n",
    "\n",
    "mlflow.sklearn.save_model(custom_model, model_path)\n",
    "\n",
    "print(f\"Meilleur modèle sauvegardé dans: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without training\n",
    "\n",
    "import pickle\n",
    "with open('C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\06_models\\\\model.pkl', 'rb') as f:\n",
    "    best_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # Assess model\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold, optimal_threshold2, manual_threshold, f_beta = assess_model(best_model, X_test, y_test)\n",
    "\n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"LGBM hyperparameters tuning\", \"second try\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    # mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns.tolist()))\n",
    "\n",
    "    # Log metrics (accuracy and AUC-ROC)\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        \"Optimal Threshold 2\": optimal_threshold2,\n",
    "        \"Manual Threshold\": manual_threshold,\n",
    "        \"F beta score\": f_beta,\n",
    "        })\n",
    "\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
