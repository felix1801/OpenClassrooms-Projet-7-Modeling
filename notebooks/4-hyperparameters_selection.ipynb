{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, precision_recall_curve, auc, confusion_matrix, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\04_feature\\\\second_features_selection.csv\"\n",
    "artifact_path = \"C:\\\\Users\\\\Z478SG\\\\Desktop\\\\Ecole\\\\OpenClassrooms-Projet-7\\\\modeling\\\\data\\\\06_models\"\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 18\n",
    "cost_fn = 10\n",
    "cost_fp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set experiment and tracking URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns/534552444611581124', creation_time=1725124865324, experiment_id='534552444611581124', last_update_time=1725124865324, lifecycle_stage='active', name='Credit_Scoring_Model', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(uri=\"file:///C:/Users/Z478SG/Desktop/Ecole/OpenClassrooms-Projet-7/modeling/mlruns\")\n",
    "mlflow.set_experiment(\"Credit_Scoring_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and select only EXT_SOURCE_2 columns\n",
    "raw_data = pd.read_csv(data_path)\n",
    "data = raw_data.astype('float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the 50 000 first rows for testing\n",
    "# data = data[:50000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(\"TARGET\", axis=1)\n",
    "y = data[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and search best parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, param_grid):\n",
    "    # Initialisation du modèle\n",
    "    model = lgb.LGBMClassifier()\n",
    "\n",
    "    # Initialisation de GridSearchCV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a work cost function\n",
    "def cost_metric(y_true, y_pred, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    cost = (fn * cost_fn) + (fp * cost_fp)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(y_true, y_proba, cost_fn=cost_fn, cost_fp=cost_fp):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    costs = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        cost = cost_metric(y_true, y_pred, cost_fn, cost_fp)\n",
    "        costs.append(cost)\n",
    "    optimal_idx = np.argmin(costs)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    return optimal_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(best_model, X_test, y_test):\n",
    "    # Prédictions sur l'ensemble de test\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    optimal_threshold = optimize_threshold(y_test, y_proba)\n",
    "    y_pred = (y_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    auc_pr = auc(recall, precision) # Area Under the Precision-Recall Curve\n",
    "    cost = cost_metric(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred) # F1 score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log experiment data into MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.334168964652795\n"
     ]
    }
   ],
   "source": [
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "base_scale_pos_weight = n_negative / n_positive\n",
    "print(base_scale_pos_weight)\n",
    "# base_scale_pos_weight = 11.334168964652795"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General advice for efficient testing:\n",
    "\n",
    "Use a smaller subset of your data for initial tests to speed up the process.  \n",
    "Focus on parameters that typically have the most impact first: learning_rate, num_leaves, and n_estimators.\n",
    "Keep other parameters at their default values while testing these key parameters.  \n",
    "Once you've found good values for the key parameters, move on to fine-tuning the others.  \n",
    "Remember that parameters can interact with each other, so you may need to revisit some parameters after changing others.  \n",
    "\n",
    "By following this strategy, you can quickly narrow down the most promising parameter values with just two initial tests for each parameter. This focused approach will help you efficiently find a good set of parameters for your LightGBM model, considering your specific requirements for handling imbalanced data and asymmetric misclassification costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create param grid with 2 or 3 options on each for LGBMClassifier\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 127, ], # 255\n",
    "    'learning_rate': [0.01, 0.1, ], # 0.3\n",
    "    'n_estimators': [100, 500, ], # 1000\n",
    "    # 'max_depth': [6, 12, ], # 20\n",
    "    # 'min_child_samples': [20, 50, ], # 100\n",
    "    # 'subsample': [0.6, 0.8, ], # 1.0\n",
    "    # 'colsample_bytree': [0.6, 0.8, ], # 1.0\n",
    "    # 'reg_alpha': [0, 1, ], # 10\n",
    "    # 'reg_lambda': [0, 1, ], # 10\n",
    "    'scale_pos_weight': [\n",
    "        base_scale_pos_weight, \n",
    "        base_scale_pos_weight * 5, \n",
    "        # base_scale_pos_weight * 10,\n",
    "        ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    ## Run experiment\n",
    "    # Train model\n",
    "    grid_search = train_model(X_train, y_train, param_grid)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Assess model\n",
    "    accuracy, auc_roc, cost, auc_pr, f1, optimal_threshold = assess_model(best_model, X_test, y_test)\n",
    "\n",
    "    ## Log data\n",
    "    # Set a tag that (key, value) we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"LGBM hyperparameters tuning\", \"first try\")\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"test_size\": test_size,\n",
    "        \"random_state\": random_state,\n",
    "        \"cost_fn\": cost_fn,\n",
    "        \"cost_fp\": cost_fp,\n",
    "        })\n",
    "    grid_params_with_suffix = {f\"{k}_tested\": v for k, v in param_grid.items()}\n",
    "    mlflow.log_params(grid_params_with_suffix)\n",
    "\n",
    "    mlflow.log_params(grid_search.best_params_)\n",
    "\n",
    "    mlflow.log_param(\"columns\", str(X_train.columns))\n",
    "\n",
    "    # Log metrics (accuracy and AUC-ROC)\n",
    "    mlflow.log_metrics({\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"AUC-ROC\": auc_roc,\n",
    "        \"Cost\": cost,\n",
    "        \"AUC-PR\": auc_pr,\n",
    "        \"F1\": f1,\n",
    "        \"Optimal Threshold\": optimal_threshold,\n",
    "        })\n",
    "\n",
    "    # Infer the model signature (input and output schema) from the training data\n",
    "    signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "    \n",
    "    # Log model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=best_model, \n",
    "        artifact_path=artifact_path, \n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        )\n",
    "\n",
    "    # Log data path\n",
    "    mlflow.log_artifact(data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View experiments in MLFlow UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a background terminal, run:  \n",
    "\n",
    "./run_mlflow_ui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If result is good : Register the model as an official model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not registered during the run, officially register the model artifact with a specific name\n",
    "\n",
    "# result = mlflow.register_model(\n",
    "#     model_info.model_uri, \"LGBMClassifier-V1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
